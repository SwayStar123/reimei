{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from transformer.microdit import ReiMei, ReiMeiParameters\n",
    "import matplotlib.pyplot as plt\n",
    "from config import VAE_CHANNELS, DIT_S as DIT, MODELS_DIR_BASE\n",
    "from torch.amp import autocast\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from diffusers import AutoencoderDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dim = 1024\n",
    "base_heads = 16\n",
    "\n",
    "input_dim = VAE_CHANNELS\n",
    "embed_dim = 1024\n",
    "num_layers = 28\n",
    "num_heads = 16\n",
    "mlp_dim = embed_dim\n",
    "cond_embed_dim = 1 # Null for this dataset\n",
    "# pos_embed_dim = 60\n",
    "pos_embed_dim = None\n",
    "num_experts = 32\n",
    "active_experts = 2.0\n",
    "shared_experts = None\n",
    "token_mixer_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "m_d = float(embed_dim) / float(base_dim)\n",
    "\n",
    "assert (embed_dim // num_heads) == (base_dim // base_heads)\n",
    "\n",
    "params = ReiMeiParameters(\n",
    "    channels=input_dim,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    text_embed_dim=cond_embed_dim,\n",
    "    vector_embed_dim=cond_embed_dim,\n",
    "    num_experts=num_experts,\n",
    "    active_experts=active_experts,\n",
    "    shared_experts=shared_experts,\n",
    "    dropout=dropout,\n",
    "    token_mixer_layers=token_mixer_layers,\n",
    "    m_d=m_d,\n",
    ")\n",
    "DTYPE = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReiMei(params)\n",
    "# model.load_state_dict(torch.load(\"models/microdit_model_and_optimizer_epoch_40_f32.pt\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device).to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of parameters in the model\n",
    "print(\"Number of parameters in the model: \", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-f32c32-in-1.0-diffusers\", torch_dtype=DTYPE, cache_dir=f\"{MODELS_DIR_BASE}/dc_ae\", revision=\"main\").to(device, ).eval()\n",
    "# print param count\n",
    "print(sum(p.numel() for p in vae.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGLIP_HF_NAME =\"hf-hub:timm/ViT-SO400M-14-SigLIP-384\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# siglip_model, _ = create_model_from_pretrained(SIGLIP_HF_NAME, precision=\"fp16\", cache_dir=f\"{MODELS_DIR_BASE}/siglip\")\n",
    "# siglip_model = siglip_model.to(device)\n",
    "# siglip_tokenizer = get_tokenizer(SIGLIP_HF_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"A serene landscape with rolling hills under a vibrant sunset sky\",\n",
    "#     \"A futuristic cityscape with towering skyscrapers and flying cars\",\n",
    "#     \"A cozy cottage in a snowy forest with smoke coming from the chimney\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"amateur photography, a woman showing the palms of her hands\",\n",
    "#     \"A futuristic cityscape with towering skyscrapers and flying cars, where the buildings are adorned with neon lights and digital billboards, creating a dazzling display of colors against the night sky, while sleek, aerodynamic vehicles zip through the air, leaving trails of light in their wake.\",\n",
    "#     \"A cozy cottage in a snowy forest, with smoke gently rising from the chimney, surrounded by tall pine trees laden with fresh snow, their branches glistening in the soft glow of the moonlight, as the warm light from the cottage windows spills out onto the snow-covered ground, inviting and comforting.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = siglip_tokenizer(prompts, context_length=siglip_model.context_length).to(device)\n",
    "# caption_embeddings = siglip_model.encode_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_embeddings = torch.zeros((1, cond_embed_dim), device=device, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1, 32, 16, 16).to(device).to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "with autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    pred = model.sample(noise, caption_embeddings.unsqueeze(1), caption_embeddings, sample_steps=50, cfg=7.0)\n",
    "\n",
    "    pred = vae.decode(pred).sample\n",
    "    # Change range of pred from x to y to -1 to 1\n",
    "    min_val = pred.min()\n",
    "    max_val = pred.max()\n",
    "\n",
    "    pred = (pred - min_val) / (max_val - min_val)\n",
    "    pred = 2 * pred - 1\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pred_cpu = pred.cpu().to(torch.float32)\n",
    "    pred_np = pred_cpu.permute(0, 2, 3, 1).numpy()\n",
    "    pred_np = (pred_np + 1) / 2\n",
    "    pred_np = (pred_np * 255).astype(np.uint8)  # Convert to uint8\n",
    "\n",
    "    # Create figure for predictions\n",
    "    fig_pred, axes_pred = plt.subplots(3, 3, figsize=(12, 12))  # 3 rows, 3 columns\n",
    "\n",
    "    # Plot predictions\n",
    "    for i, ax in enumerate(axes_pred.flatten()):\n",
    "        if i < pred_np.shape[0]:\n",
    "            ax.imshow(pred_np[i])\n",
    "            # ax.set_title(prompts[i])  # Add this line to set the title\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
